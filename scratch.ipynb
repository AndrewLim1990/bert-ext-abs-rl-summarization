{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abstractor.train import get_training_batch\n",
    "from abstractor.utils import AbstractorModel, AbstractorModelRNN\n",
    "from abstractor.utils import obtain_initial_hidden_states\n",
    "from bert.utils import obtain_sentence_embeddings\n",
    "from bert.utils import obtain_word_embeddings\n",
    "from data.utils import load_training_dictionaries\n",
    "from extractor.utils import ExtractorModel\n",
    "from pytorch_transformers import BertModel\n",
    "from pytorch_transformers import BertTokenizer\n",
    "from rouge import Rouge\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_training_dictionaries()\n",
    "# documents, extraction_labels = get_training_batch(data, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data:\n",
    "# model = AbstractorModelRNN()\n",
    "# model_path = \"results/models/abstractor.pt\"\n",
    "# model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_documents, target_summaries = get_training_batch(data, 2)\n",
    "\n",
    "# # Obtain embeddings\n",
    "# source_document_embeddings, source_mask, source_tokens = obtain_word_embeddings(\n",
    "#     model.bert_model, model.bert_tokenizer, source_documents, static_embeddings=False\n",
    "# )\n",
    "# target_summary_embeddings, target_mask, target_tokens = obtain_word_embeddings(\n",
    "#     model.bert_model, model.bert_tokenizer, target_summaries, static_embeddings=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Obtain extraction probability for each word in vocabulary\n",
    "# extraction_probabilities, teacher_forcing = model(\n",
    "#     source_document_embeddings,\n",
    "#     target_summary_embeddings,\n",
    "#     teacher_forcing_pct=0\n",
    "# )  # (batch_size, n_target_words, vocab_size)\n",
    "\n",
    "# vals, predicted_idx = torch.topk((extraction_probabilities), k=1, dim=2)\n",
    "\n",
    "# for x in [model.bert_tokenizer.convert_ids_to_tokens(p) for p in predicted_idx.squeeze().tolist()]:\n",
    "#     print(f\"{x}\")\n",
    "#     print()\n",
    "\n",
    "\n",
    "# # Obtain extraction probability for each word in vocabulary\n",
    "# extraction_probabilities, teacher_forcing = model(\n",
    "#     source_document_embeddings,\n",
    "#     target_summary_embeddings,\n",
    "#     teacher_forcing_pct=1\n",
    "# )  # (batch_size, n_target_words, vocab_size)\n",
    "\n",
    "# vals, predicted_idx = torch.topk((extraction_probabilities), k=1, dim=2)\n",
    "\n",
    "# for x in [model.bert_tokenizer.convert_ids_to_tokens(p) for p in predicted_idx.squeeze().tolist()]:\n",
    "#     print(f\"{x}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 16950,  9215, 22953, 17854,  4232,  2787,  2000,  2507,  1037,\n",
       "         14234,  2000,  1037,  7985,  1012,  1037,  2047,  3274,  2565,  3271,\n",
       "          2014, 13445, 12996, 22291,  2015,  2005,  2416, 14234,  5022,  1012,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  1037,  6302,  1997,  1037,  2665,  3536,  5051,  9102,  3909,\n",
       "          2007,  1037, 29268,  2006,  2049,  2067,  2038,  2908, 13434,  2006,\n",
       "         10474,  1012,  1996,  3746,  2001,  5941,  2011,  5515,  8088,  3235,\n",
       "          3393,  1011,  2089,  2379,  2414,  1012,  2009, 13977,  1996, 23325,\n",
       "         15900,  1001, 29268,  5051,  9102,  1998,  2038, 18379,  3365,  2033,\n",
       "          7834,  1012,   102]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tokens = torch.tensor([[  101, 16950,  9215, 22953, 17854,  4232,  2787,  2000,  2507,  1037,\n",
    "         14234,  2000,  1037,  7985,  1012,  1037,  2047,  3274,  2565,  3271,\n",
    "          2014, 13445, 12996, 22291,  2015,  2005,  2416, 14234,  5022,  1012,\n",
    "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0],\n",
    "        [  101,  1037,  6302,  1997,  1037,  2665,  3536,  5051,  9102,  3909,\n",
    "          2007,  1037, 29268,  2006,  2049,  2067,  2038,  2908, 13434,  2006,\n",
    "         10474,  1012,  1996,  3746,  2001,  5941,  2011,  5515,  8088,  3235,\n",
    "          3393,  1011,  2089,  2379,  2414,  1012,  2009, 13977,  1996, 23325,\n",
    "         15900,  1001, 29268,  5051,  9102,  1998,  2038, 18379,  3365,  2033,\n",
    "          7834,  1012,   102]])\n",
    "\n",
    "\n",
    "\n",
    "# target_tokens = torch.flatten(target_tokens)\n",
    "\n",
    "target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['##lly bro ##uss ##ard decided to give a kidney to a stranger . a new computer program helped her donation spur transplant ##s for six kidney patients . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " 'photo of a green wood ##pe ##cker flying with a weasel on its back has gone viral on twitter . the image was snapped by amateur photographer martin le - may near london . it sparked the hash ##tag # weasel ##pe ##cker and has spawned numerous me ##mes . [SEP] [PAD] [PAD]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tokens = torch.roll(target_tokens, dims=1, shifts=-1)  # shift left\n",
    "target_tokens[:, -1] = 0\n",
    "[\" \".join(bert_tokenizer.convert_ids_to_tokens(tgt_tokens)) for tgt_tokens in target_tokens.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarization-project",
   "language": "python",
   "name": "summarization-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
