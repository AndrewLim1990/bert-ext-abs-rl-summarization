{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abstractor.train import get_training_batch as get_abstractor_training_batch\n",
    "from abstractor.utils import AbstractorModel, AbstractorModelRNN\n",
    "from abstractor.utils import obtain_initial_hidden_states\n",
    "from bert.utils import obtain_sentence_embeddings\n",
    "from bert.utils import obtain_word_embeddings\n",
    "from data.utils import load_training_dictionaries\n",
    "from extractor.train import get_training_batch as get_extractor_training_batch\n",
    "from extractor.utils import ExtractorModel\n",
    "from pytorch_transformers import BertModel\n",
    "from pytorch_transformers import BertTokenizer\n",
    "from rl_connection.utils import RLModel\n",
    "from rl_connection.train import get_training_batch as get_rl_training_batch\n",
    "from rouge import Rouge\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_training_dictionaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load extractor model:\n",
    "extractor_model = ExtractorModel(bert_tokenizer, bert_model)\n",
    "extractor_model_path = \"results/models/extractor.pt\"\n",
    "extractor_model.load_state_dict(torch.load(extractor_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, extraction_labels = get_extractor_training_batch(data, batch_size=2)\n",
    "\n",
    "sentence_embeddings, mask = obtain_sentence_embeddings(\n",
    "    extractor_model.bert_model, \n",
    "    extractor_model.bert_tokenizer, \n",
    "    documents\n",
    ")\n",
    "\n",
    "# Predict probability of extraction per sentence\n",
    "extraction_probabilities = extractor_model(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> TARGET <----\n",
      "but when rory mcilroy pulled his second shot on the eighth hole of the wgc cadillac championship into a lake friday , he might as well have been channeling the much loved adam sandler character .\n",
      "\n",
      "before continuing his round with a dropped ball , the four - time major winner launched the 3 - iron used to play the offending shot into the water as well .\n",
      "\n",
      "\n",
      "----> PREDICTION <----\n",
      "but when rory mcilroy pulled his second shot on the eighth hole of the wgc cadillac championship into a lake friday , he might as well have been channeling the much loved adam sandler character . \n",
      "\n",
      "before continuing his round with a dropped ball , the four - time major winner launched the 3 - iron used to play the offending shot into the water as well . \n",
      "\n",
      "\n",
      "\n",
      "-------\n",
      "\n",
      "\n",
      "----> TARGET <----\n",
      "( cnn ) french striker bafetimbi gomis , who has a history of fainting , said he is now \" feeling well \" after collapsing during swansea 's 3 - 2 loss at tottenham in the premier league on wednesday .\n",
      "\n",
      "the worrying incident occurred in the first half at white hart lane -- after tottenham scored in the seventh minute -- but the 29 - year - old left the pitch conscious following about five minutes of treatment . the guardian added that he was wearing an oxygen mask .\n",
      "\n",
      "\" i wanted to reassure you concerning my health , \" gomis told the website . \" it actually looks much scarier than it is physically dangerous , and i am feeling well now .\n",
      "\n",
      "almost exactly three years ago at white hart lane , then bolton midfielder fabrice muamba collapsed after suffering a cardiac arrest . he was near death , according to bolton , but survived after being treated at the london chest hospital .\n",
      "\n",
      "\n",
      "----> PREDICTION <----\n",
      "almost exactly three years ago at white hart lane , then bolton midfielder fabrice muamba collapsed after suffering a cardiac arrest . he was near death , according to bolton , but survived after being treated at the london chest hospital . \n",
      "\n",
      "( cnn ) french striker bafetimbi gomis , who has a history of fainting , said he is now \" feeling well \" after collapsing during swansea 's 3 - 2 loss at tottenham in the premier league on wednesday . \n",
      "\n",
      "the worrying incident occurred in the first half at white hart lane -- after tottenham scored in the seventh minute -- but the 29 - year - old left the pitch conscious following about five minutes of treatment . the guardian added that he was wearing an oxygen mask . \n",
      "\n",
      "gomis had similar fainting spells in france , which prompted the president of his former club , jean - michel aulas of lyon , to tell french television in 2009 : \" we ca n't not be worried , it scares you each time . \" \n",
      "\n",
      "\n",
      "\n",
      "-------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(documents)\n",
    "\n",
    "for sample_idx in range(n_samples):\n",
    "    n_to_extract = extraction_labels.sum(dim=1)[sample_idx].int() \n",
    "    ext_prob = extraction_probabilities[sample_idx] * mask[sample_idx]\n",
    "    ext_sent_indicies = torch.topk(ext_prob, k=n_to_extract)[1]\n",
    "    \n",
    "    targets = np.array(documents[sample_idx])[extraction_labels[sample_idx][:len(documents[sample_idx])].numpy().astype(bool)]\n",
    "    print(\"----> TARGET <----\")\n",
    "    for target in targets:\n",
    "        print(f\"{target}\\n\")\n",
    "    print()\n",
    "          \n",
    "    print(\"----> PREDICTION <----\")\n",
    "    for x in np.array(documents[sample_idx])[ext_sent_indicies]:\n",
    "        print(f\"{x} \\n\")\n",
    "    print(\"\\n\\n-------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data:\n",
    "abstractor_model = AbstractorModelRNN(bert_tokenizer, bert_model)\n",
    "abstractor_model_path = \"results/models/abstractor.pt\"\n",
    "abstractor_model.load_state_dict(torch.load(abstractor_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['( cnn ) french striker bafetimbi gomis , who has a history of fainting , said he is now \" feeling well \" after collapsing during swansea \\'s 3 - 2 loss at tottenham in the premier league on wednesday .', 'the worrying incident occurred in the first half at white hart lane -- after tottenham scored in the seventh minute -- but the 29 - year - old left the pitch conscious following about five minutes of treatment . the guardian added that he was wearing an oxygen mask .', '\" i wanted to reassure you concerning my health , \" gomis told the website . \" it actually looks much scarier than it is physically dangerous , and i am feeling well now .', 'almost exactly three years ago at white hart lane , then bolton midfielder fabrice muamba collapsed after suffering a cardiac arrest . he was near death , according to bolton , but survived after being treated at the london chest hospital .']]\n",
      "\n",
      "[['bafetimbi gomis collapses within 10 minutes of kickoff at tottenham .', 'but he reportedly left the pitch conscious and wearing an oxygen mask .', 'gomis later said that he was \" feeling well \"', 'the incident came three years after fabrice muamba collapsed at white hart lane .']]\n"
     ]
    }
   ],
   "source": [
    "source_documents, target_summaries = get_abstractor_training_batch(data, 1)\n",
    "\n",
    "# Obtain embeddings\n",
    "source_document_embeddings, source_mask, source_tokens = obtain_word_embeddings(\n",
    "    abstractor_model.bert_model, abstractor_model.bert_tokenizer, source_documents, static_embeddings=False\n",
    ")\n",
    "target_summary_embeddings, target_mask, target_tokens = obtain_word_embeddings(\n",
    "    abstractor_model.bert_model, abstractor_model.bert_tokenizer, target_summaries, static_embeddings=True\n",
    ")\n",
    "\n",
    "print(source_documents)\n",
    "print()\n",
    "print(target_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ba\n",
      "\n",
      "##fe\n",
      "\n",
      "##ti\n",
      "\n",
      "##mb\n",
      "\n",
      "##i\n",
      "\n",
      "collapses\n",
      "\n",
      "10\n",
      "\n",
      "minutes\n",
      "\n",
      "tottenham\n",
      "\n",
      "the\n",
      "\n",
      "pitch\n",
      "\n",
      "conscious\n",
      "\n",
      "and\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "and\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "------------\n",
      "ba\n",
      "\n",
      "##fe\n",
      "\n",
      "##ti\n",
      "\n",
      "##mb\n",
      "\n",
      "##i\n",
      "\n",
      "collapses\n",
      "\n",
      "##mis\n",
      "\n",
      "collapses\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "minutes\n",
      "\n",
      "tottenham\n",
      "\n",
      "10\n",
      "\n",
      "##mis\n",
      "\n",
      "tottenham\n",
      "\n",
      "the\n",
      "\n",
      "but\n",
      "\n",
      "the\n",
      "\n",
      "reportedly\n",
      "\n",
      "left\n",
      "\n",
      "he\n",
      "\n",
      "pitch\n",
      "\n",
      "conscious\n",
      "\n",
      "and\n",
      "\n",
      "wearing\n",
      "\n",
      "wearing\n",
      "\n",
      "oxygen\n",
      "\n",
      "mask\n",
      "\n",
      "said\n",
      "\n",
      "go\n",
      "\n",
      "said\n",
      "\n",
      "later\n",
      "\n",
      "said\n",
      "\n",
      "that\n",
      "\n",
      ".\n",
      "\n",
      "\"\n",
      "\n",
      "\"\n",
      "\n",
      "feeling\n",
      "\n",
      "well\n",
      "\n",
      "incident\n",
      "\n",
      "feeling\n",
      "\n",
      "incident\n",
      "\n",
      "after\n",
      "\n",
      "three\n",
      "\n",
      "years\n",
      "\n",
      "after\n",
      "\n",
      "fabric\n",
      "\n",
      "##e\n",
      "\n",
      "mu\n",
      "\n",
      "##am\n",
      "\n",
      "##ba\n",
      "\n",
      "collapsed\n",
      "\n",
      ".\n",
      "\n",
      "white\n",
      "\n",
      "hart\n",
      "\n",
      "white\n",
      "\n",
      ".\n",
      "\n",
      "[SEP]\n",
      "\n",
      "at\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtain extraction probability for each word in vocabulary\n",
    "extraction_probabilities, teacher_forcing = abstractor_model(\n",
    "    source_document_embeddings,\n",
    "    target_summary_embeddings,\n",
    "    teacher_forcing_pct=0\n",
    ")  # (batch_size, n_target_words, vocab_size)\n",
    "\n",
    "vals, predicted_idx = torch.topk((extraction_probabilities), k=1, dim=2)\n",
    "\n",
    "for x in [abstractor_model.bert_tokenizer.convert_ids_to_tokens(p) for p in predicted_idx.squeeze().tolist()]:\n",
    "    print(f\"{x}\")\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"------------\")\n",
    "\n",
    "\n",
    "# Obtain extraction probability for each word in vocabulary\n",
    "extraction_probabilities, teacher_forcing = abstractor_model(\n",
    "    source_document_embeddings,\n",
    "    target_summary_embeddings,\n",
    "    teacher_forcing_pct=1\n",
    ")  # (batch_size, n_target_words, vocab_size)\n",
    "\n",
    "vals, predicted_idx = torch.topk((extraction_probabilities), k=1, dim=2)\n",
    "\n",
    "for x in [abstractor_model.bert_tokenizer.convert_ids_to_tokens(p) for p in predicted_idx.squeeze().tolist()]:\n",
    "    print(f\"{x}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model = RLModel(extractor_model, abstractor_model)\n",
    "rl_model.load_state_dict(torch.load(\"results/models/rl.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that may sound like an esoteric adage , but when zully broussard selflessly decided to give one of her kidneys to a stranger , her generosity paired up with big data . it resulted in six patients receiving transplants .\n",
      "\n",
      "that changed when a computer programmer named david jacobs received a kidney transplant . he had been waiting on a deceased donor list , when a live donor came along -- someone nice enough to give away a kidney to a stranger .\n",
      "\n",
      "here 's how the super swap works , according to california pacific medical center .\n",
      "\n",
      "\n",
      "\n",
      "-------\n",
      "\n",
      "\n",
      "zu ##lly bro ##uss ##ard decided bro ##uss ##ard decided computer ##uss ##ard decided computer ##uss ##ard kidney computer ##uss ##ard kidney computer give give ##ard kidney computer give give ##ard\n",
      "\n",
      "\n",
      "\n",
      "-------\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['zully broussard decided to give a kidney to a stranger .',\n",
       "  'a new computer program helped her donation spur transplants for six kidney patients .']]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_documents, target_summaries = get_rl_training_batch(data, batch_size=1)\n",
    "\n",
    "# Obtain embeddings\n",
    "source_sentence_embeddings, source_mask = obtain_sentence_embeddings(\n",
    "    rl_model.extractor_model.bert_model,\n",
    "    rl_model.extractor_model.bert_tokenizer,\n",
    "    source_documents\n",
    ")\n",
    "stop_action_index = source_sentence_embeddings.shape[1]\n",
    "target_summary_embeddings, target_mask, target_tokens = obtain_word_embeddings(\n",
    "    rl_model.abstractor_model.bert_model,\n",
    "    rl_model.abstractor_model.bert_tokenizer,\n",
    "    target_summaries,\n",
    "    static_embeddings=True\n",
    ")\n",
    "\n",
    "# Run trajectory\n",
    "actions, log_probs, entropys, values, n_ext_sents = rl_model.sample_actions(\n",
    "    source_sentence_embeddings,\n",
    "    source_mask\n",
    ")\n",
    "\n",
    "# Obtain abstracted sentence from abstractor\n",
    "predicted_tokens, word_probabilities = rl_model.create_abstracted_sentences(\n",
    "    actions,\n",
    "    source_documents,\n",
    "    n_ext_sents=n_ext_sents,\n",
    "    teacher_forcing_pct=0,\n",
    "    target_summary_embeddings=target_summary_embeddings\n",
    ")\n",
    "\n",
    "# Look at extractions\n",
    "for art_idx, doc_sentences in enumerate(actions):\n",
    "    for sent_idx in doc_sentences[:-1]:\n",
    "        print(source_documents[art_idx][sent_idx])\n",
    "        print()\n",
    "    print(\"\\n\\n-------\\n\\n\")\n",
    "    \n",
    "# Look at abstractions\n",
    "for predicted_abstraction in predicted_tokens:\n",
    "    solution = list()\n",
    "    for token in predicted_abstraction:\n",
    "        solution.append(rl_model.abstractor_model.bert_tokenizer.ids_to_tokens[int(token)])\n",
    "    print(\" \".join(solution))\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "print(\"-------\\n\\n\")\n",
    "target_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3],\n",
    "])\n",
    "a = a.unsqueeze(1)\n",
    "\n",
    "b = torch.tensor([\n",
    "    [10, 10, 10],\n",
    "    [20, 20, 20],\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[11, 12, 13],\n",
      "         [21, 22, 23]],\n",
      "\n",
      "        [[11, 12, 13],\n",
      "         [21, 22, 23]],\n",
      "\n",
      "        [[11, 12, 13],\n",
      "         [21, 22, 23]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 3])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a + b\n",
    "print(c)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "label_indicies_dict = defaultdict(list)\n",
    "for row, pos in zip(b, c):\n",
    "    label_indicies_dict[row.item()].append(pos.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {0: [0], 1: [1, 2], 2: [0, 1]})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_indicies_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][[0, 0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(7, 3)\n",
    "b = torch.rand(4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4570, 0.0248, 0.8170],\n",
      "        [0.7475, 0.7047, 0.1016],\n",
      "        [0.4365, 0.9295, 0.2564],\n",
      "        [0.7821, 0.1784, 0.2322],\n",
      "        [0.6998, 0.4841, 0.0119],\n",
      "        [0.6714, 0.4237, 0.2638],\n",
      "        [0.8909, 0.6114, 0.3419]])\n",
      "\n",
      "tensor([[0.7274, 0.1380, 0.3910],\n",
      "        [0.6914, 0.3259, 0.6325],\n",
      "        [0.6592, 0.5140, 0.6638],\n",
      "        [0.7398, 0.5345, 0.7570]])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.1843, 0.1628, 1.2079],\n",
      "         [1.1484, 0.3506, 1.4494],\n",
      "         [1.1162, 0.5387, 1.4807],\n",
      "         [1.1967, 0.5593, 1.5740]],\n",
      "\n",
      "        [[1.4748, 0.8427, 0.4925],\n",
      "         [1.4389, 1.0306, 0.7340],\n",
      "         [1.4067, 1.2187, 0.7653],\n",
      "         [1.4873, 1.2392, 0.8586]],\n",
      "\n",
      "        [[1.1638, 1.0675, 0.6474],\n",
      "         [1.1279, 1.2554, 0.8889],\n",
      "         [1.0957, 1.4435, 0.9202],\n",
      "         [1.1762, 1.4641, 1.0135]],\n",
      "\n",
      "        [[1.5094, 0.3164, 0.6232],\n",
      "         [1.4735, 0.5042, 0.8647],\n",
      "         [1.4413, 0.6923, 0.8960],\n",
      "         [1.5218, 0.7129, 0.9892]],\n",
      "\n",
      "        [[1.4271, 0.6221, 0.4029],\n",
      "         [1.3912, 0.8099, 0.6444],\n",
      "         [1.3590, 0.9980, 0.6757],\n",
      "         [1.4396, 1.0186, 0.7690]],\n",
      "\n",
      "        [[1.3987, 0.5617, 0.6548],\n",
      "         [1.3628, 0.7496, 0.8963],\n",
      "         [1.3306, 0.9377, 0.9276],\n",
      "         [1.4111, 0.9582, 1.0209]],\n",
      "\n",
      "        [[1.6183, 0.7494, 0.7329],\n",
      "         [1.5823, 0.9373, 0.9744],\n",
      "         [1.5502, 1.1254, 1.0057],\n",
      "         [1.6307, 1.1459, 1.0990]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 3])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.unsqueeze(1) + b\n",
    "print(c)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5932, 1.2935, 0.9342],\n",
       "        [0.9397, 0.9672, 1.2095],\n",
       "        [1.1525, 0.9470, 1.1873],\n",
       "        [1.6448, 1.3081, 0.5084]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0] + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarization-project",
   "language": "python",
   "name": "summarization-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
