{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4345\n",
      "eng 2803\n",
      "['il est entre a cette universite .', 'he is enrolled at that university .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one french citizen , one belgian and three malians were killed in the attack in the capital of bamako , said gabriel toure , director of a local hospital .\n",
      "\n",
      "a north african jihadist group , al - murabitun , claimed responsibility for the attack , according to mauritanian news agency al akhbar . the purported claim came in an audio message in which the group said it carried out the attack in retaliation for the killing of one of its leaders , al akhbar said .\n",
      "\n",
      "\" al - murabitun may be considered a regional competitor to al - qaeda in the islamic maghreb ( aqim ) , \" according to the jamestown foundation , a washington - based research and analysis firm . the u.s. state department said in january that al - murabitun is a \" newly - formed \" militant group that has presence in northern mali .\n",
      "\n",
      "\n",
      "---------------------------\n",
      "\n",
      "\n",
      "\" al - murabitun may be considered a regional competitor to al - qaeda in the islamic maghreb ( aqim ) , \" according to the jamestown foundation , a washington - based research and analysis firm . the u.s. state department said in january that al - murabitun is a \" newly - formed \" militant group that has presence in northern mali .\n",
      "\n",
      "a north african jihadist group , al - murabitun , claimed responsibility for the attack , according to mauritanian news agency al akhbar . the purported claim came in an audio message in which the group said it carried out the attack in retaliation for the killing of one of its leaders , al akhbar said .\n",
      "\n",
      "one french citizen , one belgian and three malians were killed in the attack in the capital of bamako , said gabriel toure , director of a local hospital .\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n",
      "( cnn ) french striker bafetimbi gomis , who has a history of fainting , said he is now \" feeling well \" after collapsing during swansea 's 3 - 2 loss at tottenham in the premier league on wednesday .\n",
      "\n",
      "the worrying incident occurred in the first half at white hart lane -- after tottenham scored in the seventh minute -- but the 29 - year - old left the pitch conscious following about five minutes of treatment . the guardian added that he was wearing an oxygen mask .\n",
      "\n",
      "\" i wanted to reassure you concerning my health , \" gomis told the website . \" it actually looks much scarier than it is physically dangerous , and i am feeling well now .\n",
      "\n",
      "almost exactly three years ago at white hart lane , then bolton midfielder fabrice muamba collapsed after suffering a cardiac arrest . he was near death , according to bolton , but survived after being treated at the london chest hospital .\n",
      "\n",
      "\n",
      "---------------------------\n",
      "\n",
      "\n",
      "( cnn ) french striker bafetimbi gomis , who has a history of fainting , said he is now \" feeling well \" after collapsing during swansea 's 3 - 2 loss at tottenham in the premier league on wednesday .\n",
      "\n",
      "the worrying incident occurred in the first half at white hart lane -- after tottenham scored in the seventh minute -- but the 29 - year - old left the pitch conscious following about five minutes of treatment . the guardian added that he was wearing an oxygen mask .\n",
      "\n",
      "\" i wanted to reassure you concerning my health , \" gomis told the website . \" it actually looks much scarier than it is physically dangerous , and i am feeling well now .\n",
      "\n",
      "almost exactly three years ago at white hart lane , then bolton midfielder fabrice muamba collapsed after suffering a cardiac arrest . he was near death , according to bolton , but survived after being treated at the london chest hospital .\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n",
      "but when rory mcilroy pulled his second shot on the eighth hole of the wgc cadillac championship into a lake friday , he might as well have been channeling the much loved adam sandler character .\n",
      "\n",
      "before continuing his round with a dropped ball , the four - time major winner launched the 3 - iron used to play the offending shot into the water as well .\n",
      "\n",
      "\n",
      "---------------------------\n",
      "\n",
      "\n",
      "but when rory mcilroy pulled his second shot on the eighth hole of the wgc cadillac championship into a lake friday , he might as well have been channeling the much loved adam sandler character .\n",
      "\n",
      "before continuing his round with a dropped ball , the four - time major winner launched the 3 - iron used to play the offending shot into the water as well .\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n",
      "\" weasels will go for anything that looks like food -- they 've got a high metabolism and they 've got to eat a lot , \" she said . \" it does n't surprise me that a weasel took a punt -- i 've seen a photo of a weasel charging a group of sparrows , they 're very hungry animals . \"\n",
      "\n",
      "weasels would not normally target green woodpeckers , pacheco said -- their predators are normally the size of a stoat or larger . but the birds are known to spend a fair amount of time on the ground pulling up worms and hunting insects .\n",
      "\n",
      "the pluckiness of the weasel spawned a number of parodies on twitter , with manipulated images showing the creature in turn being ridden by russian president vladimir putin , popstar miley cyrus , football star john terry -- and even what appears to be a dog red panda dressed in a darth vader costume . ( update : twitter has now educated us on the difference between a dog and a red panda . sorry , darth ! )\n",
      "\n",
      "\n",
      "---------------------------\n",
      "\n",
      "\n",
      "the pluckiness of the weasel spawned a number of parodies on twitter , with manipulated images showing the creature in turn being ridden by russian president vladimir putin , popstar miley cyrus , football star john terry -- and even what appears to be a dog red panda dressed in a darth vader costume . ( update : twitter has now educated us on the difference between a dog and a red panda . sorry , darth ! )\n",
      "\n",
      "weasels would not normally target green woodpeckers , pacheco said -- their predators are normally the size of a stoat or larger . but the birds are known to spend a fair amount of time on the ground pulling up worms and hunting insects .\n",
      "\n",
      "\" weasels will go for anything that looks like food -- they 've got a high metabolism and they 've got to eat a lot , \" she said . \" it does n't surprise me that a weasel took a punt -- i 've seen a photo of a weasel charging a group of sparrows , they 're very hungry animals . \"\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n",
      "newtown police say cayman was last seen wearing a gray down winter jacket , black ski pants and hiking boots . he could be in the radnor - wayne area , roughly 20 miles from philadelphia , or may have purchased a train ticket to philadelphia , according to an alert posted on facebook .\n",
      "\n",
      "a message to families from the head of the shipley school , which cayman attends , read in part : \" cayman 's sister savannah is in ninth grade at shipley and his parents , farid and becky , are terrific people . they have contacted police and are aware that we are sending you this email . we hope that cayman is ok and are saying our prayers . \"\n",
      "\n",
      "\n",
      "---------------------------\n",
      "\n",
      "\n",
      "newtown police say cayman was last seen wearing a gray down winter jacket , black ski pants and hiking boots . he could be in the radnor - wayne area , roughly 20 miles from philadelphia , or may have purchased a train ticket to philadelphia , according to an alert posted on facebook .\n",
      "\n",
      "a message to families from the head of the shipley school , which cayman attends , read in part : \" cayman 's sister savannah is in ninth grade at shipley and his parents , farid and becky , are terrific people . they have contacted police and are aware that we are sending you this email . we hope that cayman is ok and are saying our prayers . \"\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n",
      "( cnn ) my vote for father of the year goes to curt schilling . the former major league baseball pitcher recently fired off a series of fastballs and mowed down a group of twitter trolls who made the mistake of tweeting vulgar and sexually - explicit comments about schilling 's teenage daughter .\n",
      "\n",
      "finally , it 's worth reminding everyone that freedom of expression does not mean freedom from rules , standards , and expectations that should guide your behavior . there are things you do n't say . there are boundaries , ways that we expect you to behave so you do n't terrorize other people or bring shame upon yourself , your friends , and your family . if you do n't have social skills , you do n't belong on social media .\n",
      "\n",
      "\n",
      "---------------------------\n",
      "\n",
      "\n",
      "( cnn ) my vote for father of the year goes to curt schilling . the former major league baseball pitcher recently fired off a series of fastballs and mowed down a group of twitter trolls who made the mistake of tweeting vulgar and sexually - explicit comments about schilling 's teenage daughter .\n",
      "\n",
      "finally , it 's worth reminding everyone that freedom of expression does not mean freedom from rules , standards , and expectations that should guide your behavior . there are things you do n't say . there are boundaries , ways that we expect you to behave so you do n't terrorize other people or bring shame upon yourself , your friends , and your family . if you do n't have social skills , you do n't belong on social media .\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n",
      "for $ 89 , self - styled entrepreneur kyle waring will ship you 6 pounds of boston - area snow in an insulated styrofoam box -- enough for 10 to 15 snowballs , he says .\n",
      "\n",
      "according to boston.com , it all started a few weeks ago , when waring and his wife were shoveling deep snow from their yard in manchester - by - the - sea , a coastal suburb north of boston . he joked about shipping the stuff to friends and family in warmer states , and an idea was born .\n",
      "\n",
      "\n",
      "---------------------------\n",
      "\n",
      "\n",
      "according to boston.com , it all started a few weeks ago , when waring and his wife were shoveling deep snow from their yard in manchester - by - the - sea , a coastal suburb north of boston . he joked about shipping the stuff to friends and family in warmer states , and an idea was born .\n",
      "\n",
      "for $ 89 , self - styled entrepreneur kyle waring will ship you 6 pounds of boston - area snow in an insulated styrofoam box -- enough for 10 to 15 snowballs , he says .\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n",
      "\" weasels will go for anything that looks like food -- they 've got a high metabolism and they 've got to eat a lot , \" she said . \" it does n't surprise me that a weasel took a punt -- i 've seen a photo of a weasel charging a group of sparrows , they 're very hungry animals . \"\n",
      "\n",
      "weasels would not normally target green woodpeckers , pacheco said -- their predators are normally the size of a stoat or larger . but the birds are known to spend a fair amount of time on the ground pulling up worms and hunting insects .\n",
      "\n",
      "the pluckiness of the weasel spawned a number of parodies on twitter , with manipulated images showing the creature in turn being ridden by russian president vladimir putin , popstar miley cyrus , football star john terry -- and even what appears to be a dog red panda dressed in a darth vader costume . ( update : twitter has now educated us on the difference between a dog and a red panda . sorry , darth ! )\n",
      "\n",
      "\n",
      "---------------------------\n",
      "\n",
      "\n",
      "the pluckiness of the weasel spawned a number of parodies on twitter , with manipulated images showing the creature in turn being ridden by russian president vladimir putin , popstar miley cyrus , football star john terry -- and even what appears to be a dog red panda dressed in a darth vader costume . ( update : twitter has now educated us on the difference between a dog and a red panda . sorry , darth ! )\n",
      "\n",
      "weasels would not normally target green woodpeckers , pacheco said -- their predators are normally the size of a stoat or larger . but the birds are known to spend a fair amount of time on the ground pulling up worms and hunting insects .\n",
      "\n",
      "\" weasels will go for anything that looks like food -- they 've got a high metabolism and they 've got to eat a lot , \" she said . \" it does n't surprise me that a weasel took a punt -- i 've seen a photo of a weasel charging a group of sparrows , they 're very hungry animals . \"\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n",
      "but when rory mcilroy pulled his second shot on the eighth hole of the wgc cadillac championship into a lake friday , he might as well have been channeling the much loved adam sandler character .\n",
      "\n",
      "before continuing his round with a dropped ball , the four - time major winner launched the 3 - iron used to play the offending shot into the water as well .\n",
      "\n",
      "\n",
      "---------------------------\n",
      "\n",
      "\n",
      "but when rory mcilroy pulled his second shot on the eighth hole of the wgc cadillac championship into a lake friday , he might as well have been channeling the much loved adam sandler character .\n",
      "\n",
      "before continuing his round with a dropped ball , the four - time major winner launched the 3 - iron used to play the offending shot into the water as well .\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n",
      "according to phil rawlins , co-primary owner and president of the new mls franchise , orlando city soccer club , \" the industry and the game itself has moved on dramatically \" in the u.s. . he believes what would equal 50 years growth in most other industries has been experienced in the first two decades of the mls .\n",
      "\n",
      "a report by forbes at the end of 2013 , meanwhile , claimed that only 10 out of 19 mls teams were profitable . and as recently as this week , mls players looked like they could be going on strike over wages and the right of players to become free agents when their contracts end .\n",
      "\n",
      "this includes promising generation adidas players who enter the mls through the draft systems before completing their college education . homegrown players from club 's development academies are also exempt as are a maximum of three designated players ( dps ) , usually stellar international names whose wages and transfer fees will be covered by club owners or sponsors .\n",
      "\n",
      "\n",
      "---------------------------\n",
      "\n",
      "\n",
      "according to phil rawlins , co-primary owner and president of the new mls franchise , orlando city soccer club , \" the industry and the game itself has moved on dramatically \" in the u.s. . he believes what would equal 50 years growth in most other industries has been experienced in the first two decades of the mls .\n",
      "\n",
      "a report by forbes at the end of 2013 , meanwhile , claimed that only 10 out of 19 mls teams were profitable . and as recently as this week , mls players looked like they could be going on strike over wages and the right of players to become free agents when their contracts end .\n",
      "\n",
      "this includes promising generation adidas players who enter the mls through the draft systems before completing their college education . homegrown players from club 's development academies are also exempt as are a maximum of three designated players ( dps ) , usually stellar international names whose wages and transfer fees will be covered by club owners or sponsors .\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(documents):\n",
    "    label_idx = extraction_labels[i].numpy()[:len(doc)]\n",
    "    label_idx = label_idx.astype(bool)\n",
    "    \n",
    "    n_summary_sentences = label_idx.sum()\n",
    "    \n",
    "    target = np.array(doc)[label_idx]\n",
    "    prediction = np.array(doc)[predicted_idx[i]][:n_summary_sentences]\n",
    "    \n",
    "    print(\"\\n\\n\".join(target.tolist()))\n",
    "    print(\"\\n\\n---------------------------\\n\\n\")\n",
    "    print(\"\\n\\n\".join(prediction.tolist()))\n",
    "    \n",
    "    print(\"\\n\\n-----------------------------------------------------\")\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"-----------------------------------------------------\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__() \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        attn_embedding = self.attn(torch.cat((embedded[0], hidden[0]), 1))\n",
    "        attn_score = torch.mm(attn_embedding, encoder_outputs.T)\n",
    "        attn_weights = F.softmax(attn_score, dim=1)\n",
    "        \n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 6s (- 80m 44s) (100 0%) 4.6455\n",
      "0m 10s (- 66m 20s) (200 0%) 3.5457\n",
      "0m 15s (- 62m 26s) (300 0%) 3.5876\n",
      "0m 19s (- 61m 7s) (400 0%) 3.4021\n",
      "0m 24s (- 59m 44s) (500 0%) 3.3542\n",
      "0m 28s (- 58m 56s) (600 0%) 3.3115\n",
      "0m 33s (- 58m 26s) (700 0%) 3.3098\n",
      "0m 37s (- 57m 59s) (800 1%) 3.2162\n",
      "0m 41s (- 57m 31s) (900 1%) 3.1086\n",
      "0m 46s (- 57m 24s) (1000 1%) 3.2367\n",
      "0m 51s (- 57m 8s) (1100 1%) 3.1792\n",
      "0m 55s (- 56m 48s) (1200 1%) 3.1637\n",
      "0m 59s (- 56m 30s) (1300 1%) 3.1098\n",
      "1m 4s (- 56m 35s) (1400 1%) 3.2476\n",
      "1m 9s (- 56m 31s) (1500 2%) 2.8693\n",
      "1m 13s (- 56m 34s) (1600 2%) 2.9869\n",
      "1m 18s (- 56m 33s) (1700 2%) 2.9518\n",
      "1m 23s (- 56m 33s) (1800 2%) 2.8873\n",
      "1m 28s (- 56m 33s) (1900 2%) 2.8417\n",
      "1m 32s (- 56m 29s) (2000 2%) 2.8102\n",
      "1m 37s (- 56m 22s) (2100 2%) 2.7227\n",
      "1m 41s (- 56m 10s) (2200 2%) 2.7397\n",
      "1m 46s (- 56m 0s) (2300 3%) 2.8361\n",
      "1m 51s (- 55m 59s) (2400 3%) 2.6364\n",
      "1m 55s (- 55m 59s) (2500 3%) 2.7366\n",
      "2m 0s (- 55m 52s) (2600 3%) 2.8409\n",
      "2m 4s (- 55m 42s) (2700 3%) 2.6637\n",
      "2m 9s (- 55m 40s) (2800 3%) 2.7867\n",
      "2m 14s (- 55m 44s) (2900 3%) 2.6310\n",
      "2m 18s (- 55m 32s) (3000 4%) 2.7698\n",
      "2m 23s (- 55m 32s) (3100 4%) 2.7704\n",
      "2m 28s (- 55m 33s) (3200 4%) 2.7637\n",
      "2m 33s (- 55m 37s) (3300 4%) 2.7392\n",
      "2m 38s (- 55m 36s) (3400 4%) 2.6665\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-e322968083b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mattn_decoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-29eaa6fdc28a>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         loss = train(input_tensor, target_tensor, encoder,\n\u001b[0;32m---> 19\u001b[0;31m                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-1cf703f4e2ca>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/summarization/summarization-project/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/summarization/summarization-project/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=100, plot_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4284, 0.6522, 0.0675, 0.3523],\n",
       "         [0.5527, 0.0352, 0.1129, 0.0664],\n",
       "         [0.9076, 0.0788, 0.8815, 0.4983]],\n",
       "\n",
       "        [[0.2640, 0.7987, 0.5758, 0.1889],\n",
       "         [0.2354, 0.4167, 0.0108, 0.8561],\n",
       "         [0.8388, 0.0441, 0.9029, 0.4719]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((2, 3, 4))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2581, 0.3228, 0.1799, 0.2392],\n",
       "         [0.3503, 0.2088, 0.2256, 0.2154],\n",
       "         [0.3252, 0.1420, 0.3168, 0.2160]],\n",
       "\n",
       "        [[0.2000, 0.3413, 0.2731, 0.1855],\n",
       "         [0.2058, 0.2468, 0.1644, 0.3829],\n",
       "         [0.3114, 0.1407, 0.3321, 0.2158]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = F.softmax(a, dim=2)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0.2581, 0.3228, 0.1799, 0.2392]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0, 2, :].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarization-project",
   "language": "python",
   "name": "summarization-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
