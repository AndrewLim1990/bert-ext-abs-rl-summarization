{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abstractor.train import get_training_batch\n",
    "from abstractor.utils import AbstractorModel, AbstractorModelRNN\n",
    "from abstractor.utils import obtain_initial_hidden_states\n",
    "from bert.utils import obtain_sentence_embeddings\n",
    "from bert.utils import obtain_word_embeddings\n",
    "from data.utils import load_training_dictionaries\n",
    "from extractor.utils import ExtractorModel\n",
    "from pytorch_transformers import BertModel\n",
    "from pytorch_transformers import BertTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_training_dictionaries()\n",
    "documents, extraction_labels = get_training_batch(data, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data:\n",
    "model = AbstractorModelRNN()\n",
    "model_path = \"results/models/abstractor.pt\"\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_documents, target_summaries = get_training_batch(data, 2)\n",
    "\n",
    "# Obtain embeddings\n",
    "source_document_embeddings, source_mask, source_tokens = obtain_word_embeddings(\n",
    "    model.bert_model, model.bert_tokenizer, source_documents, static_embeddings=False\n",
    ")\n",
    "target_summary_embeddings, target_mask, target_tokens = obtain_word_embeddings(\n",
    "    model.bert_model, model.bert_tokenizer, target_summaries, static_embeddings=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zu', '##lly', 'bro', '##uss', '##ard', 'decided', 'to', 'a', '.', 'a', '.', 'a', '.', 'a', '.', 'a', '.', 'a', '.', 'a', '.', 'a', '.', 'a', '.', 'a', '.', 'a', '.', 'a', '.', 'a', '.', 'a', '.', 'a', '.', 'a', '.', 'a', '.', 'a', '.', 'a', '.', 'a', '.', 'a', '.', 'a', '.', 'a', '.']\n",
      "\n",
      "['a', 'green', 'wood', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel', '##cker', 'weasel']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtain extraction probability for each word in vocabulary\n",
    "extraction_probabilities, teacher_forcing = model(\n",
    "    source_document_embeddings,\n",
    "    target_summary_embeddings,\n",
    "    teacher_forcing_pct=0\n",
    ")  # (batch_size, n_target_words, vocab_size)\n",
    "\n",
    "vals, predicted_idx = torch.topk((extraction_probabilities), k=1, dim=2)\n",
    "\n",
    "for x in [model.bert_tokenizer.convert_ids_to_tokens(p) for p in predicted_idx.squeeze().tolist()]:\n",
    "    print(f\"{x}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zu', '##lly', 'bro', '##uss', '##ard', 'decided', 'to', 'a', 'a', '.', 'to', 'a', '.', '.', 'a', '.', 'computer', 'program', 'helped', 'her', 'donation', 'spur', 'transplant', '##s', 'for', 'six', 'kidney', 'patients', '.', '[SEP]', 'computer', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney']\n",
      "\n",
      "['a', 'green', 'of', 'a', 'green', 'wood', '##cker', '##cker', 'weasel', 'with', 'a', 'weasel', '##cker', 'its', 'back', 'has', 'weasel', 'viral', 'on', 'image', '.', 'has', 'image', 'was', 'snapped', 'by', '[SEP]', '.', '.', 'le', '-', 'may', 'london', 'london', '.', '[SEP]', '-', 'le', 'hash', '#', '#', '.', 'le', 'le', '.', '##tag', 'spawned', 'numerous', 'numerous', '##mes', '.', '[SEP]', 'london']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtain extraction probability for each word in vocabulary\n",
    "extraction_probabilities, teacher_forcing = model(\n",
    "    source_document_embeddings,\n",
    "    target_summary_embeddings,\n",
    "    teacher_forcing_pct=1\n",
    ")  # (batch_size, n_target_words, vocab_size)\n",
    "\n",
    "vals, predicted_idx = torch.topk((extraction_probabilities), k=1, dim=2)\n",
    "\n",
    "for x in [model.bert_tokenizer.convert_ids_to_tokens(p) for p in predicted_idx.squeeze().tolist()]:\n",
    "    print(f\"{x}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'zu', '##lly', 'bro', '##uss', '##ard', 'decided', 'to', 'give', 'a', 'kidney', 'to', 'a', 'stranger', '.', 'a', 'new', 'computer', 'program', 'helped', 'her', 'donation', 'spur', 'transplant', '##s', 'for', 'six', 'kidney', 'patients', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "['[CLS]', 'a', 'photo', 'of', 'a', 'green', 'wood', '##pe', '##cker', 'flying', 'with', 'a', 'weasel', 'on', 'its', 'back', 'has', 'gone', 'viral', 'on', 'twitter', '.', 'the', 'image', 'was', 'snapped', 'by', 'amateur', 'photographer', 'martin', 'le', '-', 'may', 'near', 'london', '.', 'it', 'sparked', 'the', 'hash', '##tag', '#', 'weasel', '##pe', '##cker', 'and', 'has', 'spawned', 'numerous', 'me', '##mes', '.', '[SEP]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = [[model.bert_tokenizer.ids_to_tokens[token] for token in sentence] for sentence in target_tokens.tolist()]\n",
    "\n",
    "for l in labels:\n",
    "    print(l)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarization-project",
   "language": "python",
   "name": "summarization-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
