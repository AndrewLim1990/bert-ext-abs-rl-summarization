{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abstractor.train import get_training_batch as get_abstractor_training_batch\n",
    "from abstractor.utils import AbstractorModel, AbstractorModelRNN\n",
    "from abstractor.utils import obtain_initial_hidden_states\n",
    "from bert.utils import obtain_sentence_embeddings\n",
    "from bert.utils import obtain_word_embeddings\n",
    "from data.utils import load_training_dictionaries\n",
    "from extractor.train import get_training_batch as get_extractor_training_batch\n",
    "from extractor.utils import ExtractorModel\n",
    "from pytorch_transformers import BertModel\n",
    "from pytorch_transformers import BertTokenizer\n",
    "from rl_connection.utils import RLModel\n",
    "from rl_connection.train import get_training_batch as get_rl_training_batch\n",
    "from rouge import Rouge\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_training_dictionaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load extractor model:\n",
    "extractor_model = ExtractorModel(bert_tokenizer, bert_model)\n",
    "extractor_model_path = \"results/models/extractor.pt\"\n",
    "extractor_model.load_state_dict(torch.load(extractor_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, extraction_labels = get_extractor_training_batch(data, batch_size=2)\n",
    "\n",
    "sentence_embeddings, mask = obtain_sentence_embeddings(\n",
    "    extractor_model.bert_model, \n",
    "    extractor_model.bert_tokenizer, \n",
    "    documents\n",
    ")\n",
    "\n",
    "# Predict probability of extraction per sentence\n",
    "extraction_probabilities = extractor_model(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> TARGET <----\n",
      "\" weasels will go for anything that looks like food -- they 've got a high metabolism and they 've got to eat a lot , \" she said . \" it does n't surprise me that a weasel took a punt -- i 've seen a photo of a weasel charging a group of sparrows , they 're very hungry animals . \"\n",
      "\n",
      "weasels would not normally target green woodpeckers , pacheco said -- their predators are normally the size of a stoat or larger . but the birds are known to spend a fair amount of time on the ground pulling up worms and hunting insects .\n",
      "\n",
      "the pluckiness of the weasel spawned a number of parodies on twitter , with manipulated images showing the creature in turn being ridden by russian president vladimir putin , popstar miley cyrus , football star john terry -- and even what appears to be a dog red panda dressed in a darth vader costume . ( update : twitter has now educated us on the difference between a dog and a red panda . sorry , darth ! )\n",
      "\n",
      "\n",
      "----> PREDICTION <----\n",
      "the pluckiness of the weasel spawned a number of parodies on twitter , with manipulated images showing the creature in turn being ridden by russian president vladimir putin , popstar miley cyrus , football star john terry -- and even what appears to be a dog red panda dressed in a darth vader costume . ( update : twitter has now educated us on the difference between a dog and a red panda . sorry , darth ! ) \n",
      "\n",
      "\" weasels will go for anything that looks like food -- they 've got a high metabolism and they 've got to eat a lot , \" she said . \" it does n't surprise me that a weasel took a punt -- i 've seen a photo of a weasel charging a group of sparrows , they 're very hungry animals . \" \n",
      "\n",
      "weasels would not normally target green woodpeckers , pacheco said -- their predators are normally the size of a stoat or larger . but the birds are known to spend a fair amount of time on the ground pulling up worms and hunting insects . \n",
      "\n",
      "\n",
      "\n",
      "-------\n",
      "\n",
      "\n",
      "----> TARGET <----\n",
      "for $ 89 , self - styled entrepreneur kyle waring will ship you 6 pounds of boston - area snow in an insulated styrofoam box -- enough for 10 to 15 snowballs , he says .\n",
      "\n",
      "according to boston.com , it all started a few weeks ago , when waring and his wife were shoveling deep snow from their yard in manchester - by - the - sea , a coastal suburb north of boston . he joked about shipping the stuff to friends and family in warmer states , and an idea was born .\n",
      "\n",
      "\n",
      "----> PREDICTION <----\n",
      "according to boston.com , it all started a few weeks ago , when waring and his wife were shoveling deep snow from their yard in manchester - by - the - sea , a coastal suburb north of boston . he joked about shipping the stuff to friends and family in warmer states , and an idea was born . \n",
      "\n",
      "for $ 89 , self - styled entrepreneur kyle waring will ship you 6 pounds of boston - area snow in an insulated styrofoam box -- enough for 10 to 15 snowballs , he says . \n",
      "\n",
      "\n",
      "\n",
      "-------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(documents)\n",
    "\n",
    "for sample_idx in range(n_samples):\n",
    "    n_to_extract = extraction_labels.sum(dim=1)[sample_idx].int() \n",
    "    ext_prob = extraction_probabilities[sample_idx] * mask[sample_idx]\n",
    "    ext_sent_indicies = torch.topk(ext_prob, k=n_to_extract)[1]\n",
    "    \n",
    "    targets = np.array(documents[sample_idx])[extraction_labels[sample_idx][:len(documents[sample_idx])].numpy().astype(bool)]\n",
    "    print(\"----> TARGET <----\")\n",
    "    for target in targets:\n",
    "        print(f\"{target}\\n\")\n",
    "    print()\n",
    "          \n",
    "    print(\"----> PREDICTION <----\")\n",
    "    for x in np.array(documents[sample_idx])[ext_sent_indicies]:\n",
    "        print(f\"{x} \\n\")\n",
    "    print(\"\\n\\n-------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data:\n",
    "abstractor_model = AbstractorModelRNN(bert_tokenizer, bert_model)\n",
    "abstractor_model_path = \"results/models/abstractor.pt\"\n",
    "abstractor_model.load_state_dict(torch.load(abstractor_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"two american women have reportedly been arrested for carving their initials into a wall with a coin inside rome 's colosseum .\", \"cambodia 's angkor archeological park experienced its own string of nudity - related incidents this year .\", \"in february , u.s. tourists and sisters lindsey kate adams and leslie jan adams were deported after allegedly getting caught taking partially nude photos at preah khan temple , one of the sacred sites inside cambodia 's angkor complex .\"]]\n",
      "\n",
      "[['two american women arrested for carving initials into a colosseum wall .', 'meanwhile , egypt investigating russian pornography film reportedly shot at great pyramids .', \"cambodia 's angkor archeological park experienced a string of nudity - related incidents this year .\"]]\n"
     ]
    }
   ],
   "source": [
    "source_documents, target_summaries = get_abstractor_training_batch(data, 1)\n",
    "\n",
    "# Obtain embeddings\n",
    "source_document_embeddings, source_mask, source_tokens = obtain_word_embeddings(\n",
    "    abstractor_model.bert_model, abstractor_model.bert_tokenizer, source_documents, static_embeddings=False\n",
    ")\n",
    "target_summary_embeddings, target_mask, target_tokens = obtain_word_embeddings(\n",
    "    abstractor_model.bert_model, abstractor_model.bert_tokenizer, target_summaries, static_embeddings=True\n",
    ")\n",
    "\n",
    "print(source_documents)\n",
    "print()\n",
    "print(target_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zu', '##lly', 'bro', '##uss', '##ard', 'decided', 'to', 'give', 'a', 'kidney', 'for', 'give', 'a', 'kidney', 'for', 'give', 'a', 'kidney', 'for', 'give', 'for', 'give', 'for', 'give', 'for', 'give', 'for', 'give', 'for', 'give', 'for', 'give', 'for', 'give', 'for', 'give', 'for', 'give', 'for', 'give', 'for', 'give', 'for', 'give', 'for', 'give', 'for', 'give', 'for']\n",
      "\n",
      "['carving', 'initials', 'into', 'a', 'col', '##oss', '##eum', 'wall', '.', 'meanwhile', '##s', '.', 'meanwhile', '##s', '.', 'meanwhile', '##s', '.', 'meanwhile', '##s', '.', 'meanwhile', '##s', '.', 'meanwhile', '##s', '.', 'meanwhile', '##s', '.', 'meanwhile', '##s', '.', 'meanwhile', '##s', '.', 'meanwhile', '##s', '.', 'meanwhile', '##s', '.', 'meanwhile', '##s', '.', 'meanwhile', '##s', '.', 'meanwhile']\n",
      "\n",
      "------------\n",
      "['zu', '##lly', 'bro', '##uss', '##ard', 'decided', 'to', 'give', 'a', 'kidney', 'for', 'a', 'kidney', '.', 'a', 'new', 'computer', 'program', 'helped', 'her', 'donation', 'spur', 'transplant', '##s', 'for', 'six', 'kidney', 'patients', '.', '[SEP]', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "\n",
      "['carving', 'american', 'initials', 'arrested', 'for', 'carving', 'initials', '.', 'a', 'wall', '##oss', 'meanwhile', 'wall', '.', 'meanwhile', '##s', 'egypt', 'film', 'shot', '.', 'film', 'reportedly', 'shot', '.', 'great', 'archeological', '##s', 'archeological', 'cambodia', 'cambodia', 's', 'ang', '##kor', 'archeological', '##kor', 'experienced', '##s', 'string', '##s', 'nu', '##dity', '-', 'related', 'incidents', 'this', 'year', '.', '[SEP]', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtain extraction probability for each word in vocabulary\n",
    "extraction_probabilities, teacher_forcing = abstractor_model(\n",
    "    source_document_embeddings,\n",
    "    target_summary_embeddings,\n",
    "    teacher_forcing_pct=0\n",
    ")  # (batch_size, n_target_words, vocab_size)\n",
    "\n",
    "vals, predicted_idx = torch.topk((extraction_probabilities), k=1, dim=2)\n",
    "\n",
    "for x in [abstractor_model.bert_tokenizer.convert_ids_to_tokens(p) for p in predicted_idx.squeeze().tolist()]:\n",
    "    print(f\"{x}\")\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"------------\")\n",
    "\n",
    "\n",
    "# Obtain extraction probability for each word in vocabulary\n",
    "extraction_probabilities, teacher_forcing = abstractor_model(\n",
    "    source_document_embeddings,\n",
    "    target_summary_embeddings,\n",
    "    teacher_forcing_pct=1\n",
    ")  # (batch_size, n_target_words, vocab_size)\n",
    "\n",
    "vals, predicted_idx = torch.topk((extraction_probabilities), k=1, dim=2)\n",
    "\n",
    "for x in [abstractor_model.bert_tokenizer.convert_ids_to_tokens(p) for p in predicted_idx.squeeze().tolist()]:\n",
    "    print(f\"{x}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model = RLModel(extractor_model, abstractor_model)\n",
    "rl_model.load_state_dict(torch.load(\"results/models/rl.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_documents, target_summaries = get_rl_training_batch(data, batch_size=1)\n",
    "\n",
    "# Obtain embeddings\n",
    "source_sentence_embeddings, source_mask = obtain_sentence_embeddings(\n",
    "    rl_model.extractor_model.bert_model,\n",
    "    rl_model.extractor_model.bert_tokenizer,\n",
    "    source_documents\n",
    ")\n",
    "stop_action_index = source_sentence_embeddings.shape[1]\n",
    "target_summary_embeddings, target_mask, target_tokens = obtain_word_embeddings(\n",
    "    rl_model.abstractor_model.bert_model,\n",
    "    rl_model.abstractor_model.bert_tokenizer,\n",
    "    target_summaries,\n",
    "    static_embeddings=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run trajectory\n",
    "actions, log_probs, entropys, values = rl_model.sample_actions(source_sentence_embeddings, source_mask)\n",
    "\n",
    "# Obtain abstracted sentence from abstractor\n",
    "predicted_tokens, word_probabilities = rl_model.create_abstracted_sentences(\n",
    "    actions,\n",
    "    source_documents,\n",
    "    stop_action_index,\n",
    "    teacher_forcing_pct=1.0,\n",
    "    target_summary_embeddings=target_summary_embeddings\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( cnn ) my vote for father of the year goes to curt schilling . the former major league baseball pitcher recently fired off a series of fastballs and mowed down a group of twitter trolls who made the mistake of tweeting vulgar and sexually - explicit comments about schilling 's teenage daughter .\n",
      "\n",
      "these misogynistic cowards may have thought they could hide in the darkness of anonymity , the sort that many have come to expect from social media sites , where you feel free to be a despicable human being because , you think , no one will ever find out who you really are and hold you accountable for your words .\n",
      "\n",
      "finally , it 's worth reminding everyone that freedom of expression does not mean freedom from rules , standards , and expectations that should guide your behavior . there are things you do n't say . there are boundaries , ways that we expect you to behave so you do n't terrorize other people or bring shame upon yourself , your friends , and your family . if you do n't have social skills , you do n't belong on social media .\n",
      "\n",
      "\n",
      "\n",
      "-------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at extractions\n",
    "for art_idx, doc_sentences in enumerate(actions):\n",
    "    for sent_idx in doc_sentences[:-1]:\n",
    "        print(source_documents[art_idx][sent_idx])\n",
    "        print()\n",
    "    print(\"\\n\\n-------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ruben navarre ##tte ##tte sc ##hill ##ing deserves praise for taking on online hate ##rs for offensive comments about his daughter . navarre his his in his his child , sc in ##ing set a model for parenting and taught us a lesson about social media . [SEP] .\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at abstractions\n",
    "for predicted_abstraction in predicted_tokens:\n",
    "    solution = list()\n",
    "    for token in predicted_abstraction:\n",
    "        solution.append(rl_model.abstractor_model.bert_tokenizer.ids_to_tokens[int(token)])\n",
    "    print(\" \".join(solution))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ruben navarrette : schilling deserves praise for taking on online haters for offensive comments about his daughter .',\n",
       "  'navarrette : in protecting his child , schilling set a model for parenting and taught us a lesson about social media .']]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarization-project",
   "language": "python",
   "name": "summarization-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
